{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import egg.core as core\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 5, 10\n",
    "\n",
    "# For convenince and reproducibility, we set some EGG-level command line arguments here\n",
    "opts = core.init(params=['--random_seed=7', # will initialize numpy, torch, and python RNGs\n",
    "                         '--lr=1e-3',   # sets the learning rate for the selected optimizer \n",
    "                         '--batch_size=32',\n",
    "                         '--optimizer=adam'])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SEED = 7\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inputs\n",
    "\n",
    "# function to make the one hot vectors for each i_att\n",
    "def generate_one_hot_vectors(x):\n",
    "    one_hot_vectors = []\n",
    "    for i in range(x):\n",
    "        vec = np.zeros(x)  # Create a zero vector of length x\n",
    "        vec[i] = 1         # Set the i-th position to 1\n",
    "        one_hot_vectors.append(vec)\n",
    "    return one_hot_vectors\n",
    "\n",
    "# item is concat of its attributes, create all possible combos\n",
    "def concat_one_hots(one_hot_vectors, num_attributes):\n",
    "    combinations = []\n",
    "    for combo in itertools.product(one_hot_vectors, one_hot_vectors):\n",
    "        combined = np.concatenate(combo)  # Concatenate the two one-hot vectors\n",
    "        combinations.append(combined)\n",
    "    return combinations\n",
    "\n",
    "\n",
    "def create_input(num_attributes, values):\n",
    "\n",
    "    # Creating the encoder\n",
    "    input = np.array([])\n",
    "\n",
    "    if num_attributes == 'Random':\n",
    "        num_attributes = np.random.randint(2, 5)\n",
    "    elif isinstance(num_attributes, int) == \"False\":\n",
    "        raise TypeError(\"Please enter an int for num_attributes.\")\n",
    "    elif num_attributes < 2 or num_attributes > 4:\n",
    "        raise ValueError(\"Please choose value for n_attributes >= 2, <=4.\")\n",
    "\n",
    "    if values == 'Random':\n",
    "        for attribute in range(num_attributes):\n",
    "            values = np.random.randint(4, 101)\n",
    "    elif isinstance(values, int) == \"False\":\n",
    "        raise TypeError(\"Please enter an int between 4 and 100 for values.\")\n",
    "    elif values < 4 or values > 100:\n",
    "        raise ValueError(\"Please choose an int  >=4, <=100.\")\n",
    "\n",
    "    one_hots = generate_one_hot_vectors(values)\n",
    "\n",
    "    results = concat_one_hots(one_hots, num_attributes)\n",
    "    size_i = values**(num_attributes)\n",
    "\n",
    "    \n",
    "    return results, size_i, values, num_attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5041\n"
     ]
    }
   ],
   "source": [
    "dataset, size_data, n_values, n_attributes = (create_input(\"Random\", \"Random\"))\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "message_len = 10\n",
    "\n",
    "# shuffle the input data\n",
    "random.shuffle(dataset)\n",
    "\n",
    "print(dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from code compo_vs_generalization\n",
    "\n",
    "def split_train_test(dataset, p_hold_out=0.1, random_seed=7):\n",
    "\n",
    "    assert p_hold_out > 0\n",
    "    random_state = np.random.RandomState(seed=random_seed)\n",
    "\n",
    "    n = len(dataset)\n",
    "    permutation = random_state.permutation(n)\n",
    "\n",
    "    n_test = int(p_hold_out * n)\n",
    "\n",
    "    test = [dataset[i] for i in permutation[:n_test]]\n",
    "    train = [dataset[i] for i in permutation[n_test:]]\n",
    "    assert train and test\n",
    "\n",
    "    assert len(train) + len(test) == len(dataset)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_train_test(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.tensor(np.array(train).tolist())\n",
    "test_tensor = torch.tensor(np.array(test).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create alphabet\n",
    "alphabet_size = 100 \n",
    "\n",
    "def create_alphabet(alphabet_size, seed):\n",
    "    random.seed(seed)\n",
    "    symbols = set()\n",
    "    total_symbols = 63\n",
    "    while len(symbols) != total_symbols:\n",
    "        symbols.add(random.choice(string.ascii_letters + string.digits + \"#\"))\n",
    "\n",
    "\n",
    "    if alphabet_size > total_symbols:\n",
    "        alphabet = symbols.copy()\n",
    "        while len(alphabet) != alphabet_size:\n",
    "            alphabet.add(''.join(random.sample(sorted(symbols), 2)))\n",
    "\n",
    "    return list(alphabet)\n",
    "\n",
    "\n",
    "alphabet = create_alphabet(100, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class Receiver(nn.Module):\n",
    "    def __init__(self, hidden_size_receiver, n_inputs):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(hidden_size_receiver, n_inputs)\n",
    "  \n",
    "    def forward(self, rnn_output, _input = None):\n",
    "        return self.fc(rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_output = 50\n",
    "n_input = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 10\n",
    "cell = 'lstm'\n",
    "hidden_size = 142\n",
    "embed_dim = 5\n",
    "vocab_size = 10\n",
    "temp = 1.0\n",
    "hidden_size_receiver = 50\n",
    "\n",
    "sender = nn.Linear(n_input, hidden_size)\n",
    "sender = core.RnnSenderGS(sender, vocab_size=vocab_size, embed_dim=embed_dim, hidden_size=hidden_size, max_len=max_len, temperature=temp, cell=cell)\n",
    "\n",
    "receiver = core.RnnReceiverGS(Receiver(hidden_size_receiver, n_input), vocab_size=vocab_size, embed_dim=embed_dim, hidden_size=hidden_size_receiver, cell='rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(sender_input, _message, _receiver_input, receiver_output, _labels):\n",
    "    return (sender_input - receiver_output).pow(2.0).mean(dim=1), {'aux' : 0, 'labels' : None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = core.SenderReceiverRnnGS(sender, receiver, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "        {'params': game.sender.parameters(), 'lr': 1e-3},\n",
    "        {'params': game.receiver.parameters(), 'lr': 1e-2}\n",
    "    ])\n",
    "callbacks = [core.ConsoleLogger(as_json=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # Return number of rows (lists)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]  # Return the row (list) at the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor_dataset = TensorDataset(train_tensor)\n",
    "test_tensor_dataset = TensorDataset(test_tensor)\n",
    "\n",
    "test_loader = DataLoader(test_tensor_dataset, batch_size=1, shuffle=False)\n",
    "train_loader = DataLoader(train_tensor_dataset, batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b576a41eb256>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer = core.Trainer(game=game, optimizer=optimizer, train_data=train_loader,\n\u001b[0;32m      2\u001b[0m                            validation_data=test_loader)\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\franc\\Documents\\GitHub\\ANCM_EGG\\egg\\core\\trainers.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, n_epochs)\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_rest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\franc\\Documents\\GitHub\\ANCM_EGG\\egg\\core\\trainers.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmove_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[0moptimized_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m             \u001b[0mmean_rest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_add_dicts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_rest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[0moptimized_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\franc\\anaconda3\\envs\\egg36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'labels'"
     ]
    }
   ],
   "source": [
    "trainer = core.Trainer(game=game, optimizer=optimizer, train_data=train_loader,\n",
    "                           validation_data=test_loader)\n",
    "trainer.train(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # parts adapted from compo_vs_generalization, parts adjusted for this case\\nclass DiffLoss(torch.nn.Module):\\n    def __init__(self, n_attributes, n_values,\\n                 generalization=False):\\n        super().__init__()\\n        self.n_attributes = n_attributes\\n        self.n_values = n_values\\n        self.alphabet = alphabet\\n        self.message_len = message_len\\n        self.test_generalization = generalization\\n\\n    def forward(self, sender_input, _message, _receiver_input, receiver_output, labels):\\n        batch_size = sender_input.size(0)\\n        sender_input = sender_input.view(\\n            batch_size, self.n_attributes, self.n_values)\\n        receiver_output = receiver_output.view(\\n            batch_size, self.n_attributes, self.n_values)\\n        \\n        acc = (torch.sum((receiver_output.argmax(dim=-1) == sender_input.argmax(dim=-1)\\n                              ).detach(), dim=1) == self.n_attributes).float().mean()\\n        acc_or = (receiver_output.argmax(dim=-1) ==\\n                      sender_input.argmax(dim=-1)).float().mean()\\n        \\n        flattened_tensor = torch.flatten(sender_input[0])\\n        probabilities = F.softmax(flattened_tensor / 1.0, dim=0)\\n        sampled_idx = torch.multinomial(probabilities, message_len)\\n        labels = \\'\\'\\n        for char in range(len(sampled_idx)):\\n            sampled_char = alphabet[sampled_idx[char]]\\n            labels += sampled_char\\n        loss = F.cross_entropy(receiver_output, labels, reduction=\"none\").view(\\n                batch_size, self.n_attributes).mean(dim=-1)\\n\\n        return loss, {\\'acc\\': acc, \\'acc_or\\': acc_or}\\n        \\n '"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # parts adapted from compo_vs_generalization, parts adjusted for this case\n",
    "class DiffLoss(torch.nn.Module):\n",
    "    def __init__(self, n_attributes, n_values,\n",
    "                 generalization=False):\n",
    "        super().__init__()\n",
    "        self.n_attributes = n_attributes\n",
    "        self.n_values = n_values\n",
    "        self.alphabet = alphabet\n",
    "        self.message_len = message_len\n",
    "        self.test_generalization = generalization\n",
    "\n",
    "    def forward(self, sender_input, _message, _receiver_input, receiver_output, labels):\n",
    "        batch_size = sender_input.size(0)\n",
    "        sender_input = sender_input.view(\n",
    "            batch_size, self.n_attributes, self.n_values)\n",
    "        receiver_output = receiver_output.view(\n",
    "            batch_size, self.n_attributes, self.n_values)\n",
    "        \n",
    "        acc = (torch.sum((receiver_output.argmax(dim=-1) == sender_input.argmax(dim=-1)\n",
    "                              ).detach(), dim=1) == self.n_attributes).float().mean()\n",
    "        acc_or = (receiver_output.argmax(dim=-1) ==\n",
    "                      sender_input.argmax(dim=-1)).float().mean()\n",
    "        \n",
    "        flattened_tensor = torch.flatten(sender_input[0])\n",
    "        probabilities = F.softmax(flattened_tensor / 1.0, dim=0)\n",
    "        sampled_idx = torch.multinomial(probabilities, message_len)\n",
    "        labels = ''\n",
    "        for char in range(len(sampled_idx)):\n",
    "            sampled_char = alphabet[sampled_idx[char]]\n",
    "            labels += sampled_char\n",
    "        loss = F.cross_entropy(receiver_output, labels, reduction=\"none\").view(\n",
    "                batch_size, self.n_attributes).mean(dim=-1)\n",
    "\n",
    "        return loss, {'acc': acc, 'acc_or': acc_or}\n",
    "        \n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" #loss = DiffLoss(n_attributes, n_values)\\n\\ndef loss(sender_input, _message, _receiver_input, receiver_output, _labels):\\n    return F.mse_loss(sender_input, receiver_output, reduction='none').mean(dim=1), {'aux': 5.0} \""
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" #loss = DiffLoss(n_attributes, n_values)\n",
    "\n",
    "def loss(sender_input, _message, _receiver_input, receiver_output, _labels):\n",
    "    return F.mse_loss(sender_input, receiver_output, reduction='none').mean(dim=1), {'aux': 5.0} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' train_loader = DataLoader(train, batch_size=alphabet_size)\\ntest_loader = DataLoader(test, batch_size=alphabet_size) '"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" train_loader = DataLoader(train, batch_size=alphabet_size)\n",
    "test_loader = DataLoader(test, batch_size=alphabet_size) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer_labels = []\n",
    "#for item in train_loader:\n",
    "    #output_sender = sender(item[0][0])\n",
    "    #flattened_tensor = torch.flatten(output_sender[0])\n",
    "    #probabilities = F.softmax(flattened_tensor / 1.0, dim=0)\n",
    "    #sampled_idx = torch.multinomial(probabilities, message_len, replacement=True)\n",
    "    #label = ''\n",
    "    #for char in range(len(sampled_idx)):\n",
    "        #sampled_char = alphabet[sampled_idx[char]]\n",
    "        #label += sampled_char\n",
    "    #trainer_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' game = core.SenderReceiverRnnReinforce(sender, receiver, loss, sender_entropy_coeff=0.0, receiver_entropy_coeff=0.0)\\noptimizer = torch.optim.Adam(game.parameters())\\n\\ntrainer = core.Trainer(\\n    game=game, optimizer=optimizer, train_data=train_loader,\\n    validation_data=test_loader\\n    ) '"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" game = core.SenderReceiverRnnReinforce(sender, receiver, loss, sender_entropy_coeff=0.0, receiver_entropy_coeff=0.0)\n",
    "optimizer = torch.optim.Adam(game.parameters())\n",
    "\n",
    "trainer = core.Trainer(\n",
    "    game=game, optimizer=optimizer, train_data=train_loader,\n",
    "    validation_data=test_loader\n",
    "    ) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' n_epochs = 15\\n\\ninput = torch.zeros((hidden_size, max_len)).normal_()\\noptimized_loss, aux_info = game(train, labels=None)\\n\\ntrainer.train(n_epochs) '"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" n_epochs = 15\n",
    "\n",
    "input = torch.zeros((hidden_size, max_len)).normal_()\n",
    "optimized_loss, aux_info = game(train, labels=None)\n",
    "\n",
    "trainer.train(n_epochs) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "egg36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
